{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c9926df",
   "metadata": {},
   "source": [
    "# Enhanced RAG + Multi-Index Chroma Conversational Agent\n",
    "\n",
    "## A conversational AI agent built using LangChain, Chroma, and Google Gemini, capable of retrieval-augmented generation (RAG) across multiple domains with persistent memory. This agent can process PDFs, index them into domain-specific vector stores, and answer user queries using both indexed documents and tools like a calculator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b2426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced RAG + Multi-Index Chroma Conversational Agent\n",
    "- Persistent Chroma vector store (multi-collection / domain-specific)\n",
    "- HuggingFace embeddings\n",
    "- Google Gemini (ChatGoogleGenerativeAI) as LLM\n",
    "- *** FIXED: Persistent ConversationBufferMemory (Saves to JSON file) ***\n",
    "- Tools (Calculator example, RAG tool)\n",
    "- Single Agent with verbose=True that uses memory, tools, and RAG\n",
    "- Metadata using LangChain Document objects (richer metadata)\n",
    "- Incremental indexing via add_documents() with duplicate avoidance\n",
    "- Query rewriter chain (context optimizer)\n",
    "\n",
    "Instructions:\n",
    "- Set GOOGLE_API_KEY in a .env file or environment\n",
    "- pip install required packages (langchain, langchain-google-genai, chromadb, sentence-transformers, PyMuPDF, python-dotenv)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import hashlib\n",
    "import json\n",
    "import gc\n",
    "from dotenv import load_dotenv\n",
    "import fitz  # PyMuPDF\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilenames\n",
    "\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "from langchain.chains import LLMChain, RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory, ChatMessageHistory\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "# *** IMPORTS FOR PERSISTENT MEMORY ***\n",
    "from langchain.schema.messages import messages_from_dict, messages_to_dict\n",
    "\n",
    "# ===============================\n",
    "# Config / Env\n",
    "# ===============================\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "MODEL_NAME = os.getenv(\"GEMINI_MODEL\", \"gemini-2.0-flash\")\n",
    "\n",
    "# Local storage\n",
    "CHROMA_DIR = \"./chroma_db\"\n",
    "CACHE_FILE = \"./pdf_cache.json\"\n",
    "# *** File for persistent memory ***\n",
    "MEMORY_FILE = \"./conversation_history.json\"\n",
    "\n",
    "\n",
    "# Embeddings model\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# RAG settings\n",
    "DEFAULT_CHUNK_SIZE = 1000\n",
    "DEFAULT_CHUNK_OVERLAP = 100\n",
    "\n",
    "# Domain mapping (basic). Extend as needed.\n",
    "DOMAIN_KEYWORDS = {\n",
    "    \"legal\": [\"law\", \"contract\", \"agreement\", \"court\", \"legal\"],\n",
    "    \"finance\": [\"invoice\", \"payment\", \"finance\", \"budget\", \"tax\"],\n",
    "    \"research\": [\"chapter\", \"study\", \"research\", \"methodology\", \"results\"],\n",
    "    \"general\": []\n",
    "}\n",
    "\n",
    "# Safety: Ensure keys exist\n",
    "if GEMINI_API_KEY is None:\n",
    "    print(\"‚ö†Ô∏è WARNING: GOOGLE_API_KEY not found in environment. The LLM may fail at runtime if not set.\")\n",
    "\n",
    "# ===============================\n",
    "# Persistent Memory Functions\n",
    "# ===============================\n",
    "\n",
    "def save_memory(memory_object: ConversationBufferMemory):\n",
    "    \"\"\"Saves the conversation history to a JSON file.\"\"\"\n",
    "    try:\n",
    "        messages = memory_object.chat_memory.messages\n",
    "        history_dict = messages_to_dict(messages)\n",
    "        with open(MEMORY_FILE, \"w\") as f:\n",
    "            json.dump(history_dict, f, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving memory: {e}\")\n",
    "\n",
    "\n",
    "def load_memory() -> ConversationBufferMemory:\n",
    "    \"\"\"Loads conversation history from a JSON file or returns a new memory object.\"\"\"\n",
    "    if os.path.exists(MEMORY_FILE):\n",
    "        try:\n",
    "            with open(MEMORY_FILE, \"r\") as f:\n",
    "                history_dict = json.load(f)\n",
    "            \n",
    "            messages = messages_from_dict(history_dict)\n",
    "            # Create a history object and add messages\n",
    "            message_history = ChatMessageHistory(messages=messages)\n",
    "            \n",
    "            # Create the buffer memory using this pre-filled history\n",
    "            memory = ConversationBufferMemory(\n",
    "                memory_key=\"chat_history\",\n",
    "                input_key=\"input\",\n",
    "                return_messages=True,\n",
    "                chat_memory=message_history\n",
    "            )\n",
    "            print(f\"üß† Loaded persistent memory from {MEMORY_FILE} with {len(messages)} messages.\")\n",
    "            return memory\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error loading memory file, starting fresh: {e}\")\n",
    "    \n",
    "    # No file or error loading, start with new empty memory\n",
    "    print(\"üß† Starting with new, empty memory.\")\n",
    "    return ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        input_key=\"input\",\n",
    "        return_messages=True\n",
    "    )\n",
    "\n",
    "# ===============================\n",
    "# Globals\n",
    "# ===============================\n",
    "_chroma_clients: Dict[str, Chroma] = {}\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "llm = ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0.2)\n",
    "\n",
    "# Load memory from file on startup\n",
    "memory = load_memory()\n",
    "\n",
    "# Query rewriter chain\n",
    "rewrite_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=(\n",
    "        \"Rewrite the user query to be concise and focused for document retrieval. \"\n",
    "        \"Keep entities and important keywords but remove chit-chat. Return only the rewritten query.\\n\\n\"\n",
    "        \"User query: {query}\\n\\n\"\n",
    "        \"Rewritten query:\"\n",
    "    )\n",
    ")\n",
    "query_rewriter = LLMChain(llm=llm, prompt=rewrite_prompt, verbose=False)\n",
    "\n",
    "# ===============================\n",
    "# Helpers: Vector store (Chroma multi-collection)\n",
    "# ===============================\n",
    "\n",
    "def ensure_chroma_for_domain(domain: str) -> Chroma:\n",
    "    \"\"\"Return a Chroma client bound to a collection (domain). Lazily initializes and caches the client.\"\"\"\n",
    "    domain = domain or \"general\"\n",
    "    if domain in _chroma_clients:\n",
    "        return _chroma_clients[domain]\n",
    "\n",
    "    os.makedirs(CHROMA_DIR, exist_ok=True)\n",
    "    client = Chroma(persist_directory=CHROMA_DIR, embedding_function=embeddings, collection_name=domain)\n",
    "    _chroma_clients[domain] = client\n",
    "    print(f\"‚úÖ Initialized Chroma collection for domain: {domain}\")\n",
    "    return client\n",
    "\n",
    "\n",
    "def file_hash(file_path: str) -> str:\n",
    "    sha1 = hashlib.sha1()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        while chunk := f.read(8192):\n",
    "            sha1.update(chunk)\n",
    "    return sha1.hexdigest()\n",
    "\n",
    "\n",
    "def detect_domain_from_text(text: str) -> str:\n",
    "    txt = text.lower()\n",
    "    for domain, kws in DOMAIN_KEYWORDS.items():\n",
    "        for kw in kws:\n",
    "            if kw in txt:\n",
    "                return domain\n",
    "    return \"general\"\n",
    "\n",
    "\n",
    "def detect_domain_from_query(query: str) -> str:\n",
    "    q = query.lower()\n",
    "    for domain, kws in DOMAIN_KEYWORDS.items():\n",
    "        for kw in kws:\n",
    "            if kw in q:\n",
    "                return domain\n",
    "    return \"general\"\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# PDF processing with metadata and incremental indexing\n",
    "# ===============================\n",
    "\n",
    "def process_pdf(file_path: str, domain: Optional[str] = None, chunk_size: int = DEFAULT_CHUNK_SIZE, chunk_overlap: int = DEFAULT_CHUNK_OVERLAP) -> int:\n",
    "    \"\"\"Process a PDF by page, create Document objects with rich metadata and add them to the domain collection.\n",
    "    Returns number of chunks added (0 if unchanged).\n",
    "    \"\"\"\n",
    "    with fitz.open(file_path) as doc:\n",
    "        sample_text = \"\\n\".join(page.get_text() for page in doc[:min(3, len(doc))])\n",
    "\n",
    "    if domain is None:\n",
    "        domain = detect_domain_from_text(sample_text)\n",
    "\n",
    "    client = ensure_chroma_for_domain(domain)\n",
    "\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        with open(CACHE_FILE, \"r\") as f:\n",
    "            pdf_cache = json.load(f)\n",
    "    else:\n",
    "        pdf_cache = {}\n",
    "\n",
    "    h = file_hash(file_path)\n",
    "    cached_hash = pdf_cache.get(file_path)\n",
    "\n",
    "    if cached_hash == h:\n",
    "        print(f\"‚ÑπÔ∏è No changes detected in '{os.path.basename(file_path)}' (domain={domain}), skipping processing.\")\n",
    "        return 0\n",
    "\n",
    "    docs_to_add: List[Document] = []\n",
    "    doc = fitz.open(file_path)\n",
    "    for page_idx, page in enumerate(doc):\n",
    "        page_text = page.get_text()\n",
    "        if not page_text.strip():\n",
    "            continue\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        chunks = text_splitter.split_text(page_text)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            metadata = {\n",
    "                \"source\": os.path.basename(file_path),\n",
    "                \"page\": page_idx + 1,\n",
    "                \"chunk\": i,\n",
    "                \"domain\": domain\n",
    "            }\n",
    "            docs_to_add.append(Document(page_content=chunk, metadata=metadata))\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    try:\n",
    "        existing_meta = client.get(include=['metadatas'])\n",
    "        existing_mds = existing_meta.get('metadatas', [])\n",
    "    except Exception:\n",
    "        existing_mds = []\n",
    "\n",
    "    existing_signatures = set()\n",
    "    for md in existing_mds:\n",
    "        if isinstance(md, dict):\n",
    "            key = (md.get('source'), md.get('page'), md.get('chunk'))\n",
    "            existing_signatures.add(key)\n",
    "\n",
    "    filtered_docs = [d for d in docs_to_add if (d.metadata.get('source'), d.metadata.get('page'), d.metadata.get('chunk')) not in existing_signatures]\n",
    "\n",
    "    if not filtered_docs:\n",
    "        print(f\"‚ÑπÔ∏è All chunks from '{os.path.basename(file_path)}' already indexed in domain '{domain}'.\")\n",
    "        pdf_cache[file_path] = h\n",
    "        with open(CACHE_FILE, \"w\") as f:\n",
    "            json.dump(pdf_cache, f, indent=2)\n",
    "        return 0\n",
    "\n",
    "    client.add_documents(filtered_docs)\n",
    "    client.persist()\n",
    "\n",
    "    pdf_cache[file_path] = h\n",
    "    with open(CACHE_FILE, \"w\") as f:\n",
    "        json.dump(pdf_cache, f, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Processed '{os.path.basename(file_path)}' into domain '{domain}' and added {len(filtered_docs)} chunks to vector store.\")\n",
    "    return len(filtered_docs)\n",
    "\n",
    "\n",
    "def upload_and_process_pdfs():\n",
    "    Tk().withdraw()\n",
    "    pdf_files = askopenfilenames(title=\"Select PDF files\", filetypes=[(\"PDF Files\", \"*.pdf\")])\n",
    "    if not pdf_files:\n",
    "        print(\"‚ùå No PDFs selected.\")\n",
    "        return\n",
    "\n",
    "    total = 0\n",
    "    for p in pdf_files:\n",
    "        total += process_pdf(p)\n",
    "    print(f\"üîÅ Done. Total chunks added: {total}\")\n",
    "\n",
    "\n",
    "def list_stored_pdfs():\n",
    "    found = {}\n",
    "    for domain in list(DOMAIN_KEYWORDS.keys()) + [\"general\"]:\n",
    "        try:\n",
    "            client = ensure_chroma_for_domain(domain)\n",
    "            info = client.get(include=['metadatas', 'documents'])\n",
    "            mds = info.get('metadatas', [])\n",
    "            sources = set(md.get('source') for md in mds if isinstance(md, dict) and md.get('source'))\n",
    "            if sources:\n",
    "                found[domain] = sources\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if not found:\n",
    "        print(\"(No PDFs indexed yet)\")\n",
    "        return\n",
    "\n",
    "    print(\"üìÑ Indexed PDFs by domain:\")\n",
    "    for dom, sources in found.items():\n",
    "        print(f\" - {dom}:\")\n",
    "        for s in sources:\n",
    "            print(\"    -\", s)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Retrieval (Hybrid) + RAG answer with domain awareness and query rewriter\n",
    "# ===============================\n",
    "\n",
    "def hybrid_retrieve(query: str, domain: Optional[str] = None, k: int = 5, keyword_search: bool = True, show_sources: bool = True) -> List[Document]:\n",
    "    \"\"\"Performs vector similarity search on the selected domain collection + optional keyword match and returns top-k Documents.\"\"\"\n",
    "    try:\n",
    "        rewritten = query_rewriter.run(query=query).strip()\n",
    "        if rewritten:\n",
    "            use_query = rewritten\n",
    "        else:\n",
    "            use_query = query\n",
    "    except Exception:\n",
    "        use_query = query\n",
    "\n",
    "    if domain is None:\n",
    "        domain = detect_domain_from_query(query)\n",
    "\n",
    "    client = ensure_chroma_for_domain(domain)\n",
    "\n",
    "    results: List[Document] = []\n",
    "    try:\n",
    "        vector_matches = client.similarity_search(use_query, k=k)\n",
    "    except Exception:\n",
    "        try:\n",
    "            retr = client.as_retriever()\n",
    "            vector_matches = retr.get_relevant_documents(use_query)[:k]\n",
    "        except Exception as e:\n",
    "            print(\"‚ö†Ô∏è Vector search failed:\", e)\n",
    "            vector_matches = []\n",
    "\n",
    "    added_texts = set()\n",
    "    for d in vector_matches:\n",
    "        results.append(d)\n",
    "        added_texts.add(d.page_content)\n",
    "\n",
    "    if keyword_search:\n",
    "        try:\n",
    "            all_docs = client.get(include=['documents']).get('documents', [])\n",
    "            for doc_text in all_docs:\n",
    "                if use_query.lower() in doc_text.lower() and doc_text not in added_texts:\n",
    "                    results.append(Document(page_content=doc_text, metadata={\"source\": \"keyword_match\", \"domain\": domain}))\n",
    "                    added_texts.add(doc_text)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if show_sources:\n",
    "        print(f\"\\nüîç Retrieved chunks (domain={domain}):\")\n",
    "        for i, d in enumerate(results[:k], 1):\n",
    "            meta = getattr(d, \"metadata\", {}) or {}\n",
    "            preview = getattr(d, \"page_content\", str(d))\n",
    "            print(f\" {i}. Source: {meta.get('source','unknown')} | page={meta.get('page','?')} | len={len(preview)} chars\")\n",
    "\n",
    "    return results[:k]\n",
    "\n",
    "\n",
    "def get_rag_answer(query: str, domain: Optional[str] = None, k: int = 5) -> str:\n",
    "    \"\"\"Builds the RAG prompt and queries the LLM. Returns text answer.\"\"\"\n",
    "    docs = hybrid_retrieve(query, domain=domain, k=k, show_sources=True)\n",
    "    context = \"\\n\\n---\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant. Answer the user's question using ONLY the provided CONTEXT. If the answer is not contained in the context, say \"I don't know based on the provided documents.\" Do NOT hallucinate.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    if hasattr(response, \"content\"):\n",
    "        if isinstance(response.content, str):\n",
    "            return response.content\n",
    "        try:\n",
    "            return response.content[0].text\n",
    "        except Exception:\n",
    "            return str(response.content)\n",
    "    return str(response)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Tools (example: calculator and RAG tool)\n",
    "# ===============================\n",
    "\n",
    "def calculator_tool(query: str) -> str:\n",
    "    try:\n",
    "        if not re.match(r\"^[0-9\\.\\+\\-\\*\\/\\(\\) \\n]+$\", query.strip()):\n",
    "            return \"Error: Unsafe characters in expression.\"\n",
    "        return str(eval(query))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "def rag_tool(query: str) -> str:\n",
    "    domain = detect_domain_from_query(query)\n",
    "    return get_rag_answer(query, domain=domain)\n",
    "\n",
    "\n",
    "calc_tool = Tool(name=\"Calculator\", func=calculator_tool, description=\"Performs math calculations\")\n",
    "rag_tool_wrapper = Tool(name=\"RAGRetriever\", func=rag_tool, description=\"Answers questions using the indexed documents (RAG). Use this for any query about PDFs, documents, chapters, summaries, or specific content from a file.\")\n",
    "\n",
    "# ===============================\n",
    "# Initialize Agent (with tools, memory, verbose=True)\n",
    "# ===============================\n",
    "agent_llm = llm\n",
    "\n",
    "# *** FIX: Added {chat_history} placeholder to the prefix ***\n",
    "agent_kwargs = {\n",
    "    \"prefix\": \"\"\"You are a friendly conversational assistant.\n",
    "You have access to the following tools:\n",
    "1. Calculator: Performs math calculations.\n",
    "2. RAGRetriever: Answers questions using the indexed documents (RAG). Use this for any query about PDFs, documents, chapters, summaries, or specific content from a file.\n",
    "\n",
    "You also have access to the conversation history.\n",
    "If the user is just chatting, respond conversationally.\n",
    "If the user asks a question, decide if it requires a tool or if you can answer from history.\n",
    "\n",
    "Here is the conversation history:\n",
    "{chat_history}\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=[calc_tool, rag_tool_wrapper],\n",
    "    llm=agent_llm,\n",
    "    agent_type=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    max_iterations=4,\n",
    "    memory=memory,  # Memory object is now loaded from file\n",
    "    agent_kwargs=agent_kwargs,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# CLI / Main loop\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== RAG + Agent Conversational App (multi-domain Chroma) ===\")\n",
    "    while True:\n",
    "        print(\"\\nOptions:\\n1) Upload PDFs to index\\n2) List indexed PDFs\\n3) Ask a question (Agent handles RAG/Tools/Chat)\\n4) Run example queries\\n5) Clear Memory\\n6) Exit\")\n",
    "        choice = input(\"Choose: \").strip()\n",
    "        if choice == \"1\":\n",
    "            upload_and_process_pdfs()\n",
    "        elif choice == \"2\":\n",
    "            list_stored_pdfs()\n",
    "        elif choice == \"3\":\n",
    "            q = input(\"Enter question: \")\n",
    "            try:\n",
    "                answer = agent.run(q)\n",
    "                print(\"\\n== Answer ==\\n\", answer)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during agent execution: {e}\")\n",
    "            finally:\n",
    "                # *** FIX: Save memory in 'finally' to ensure it saves even on error ***\n",
    "                save_memory(memory)\n",
    "                print(\"üíæ Memory saved.\")\n",
    "        elif choice == \"4\":\n",
    "            examples = [\n",
    "                \"Hi, my name is Alex.\",\n",
    "                \"What is 25*6?\",\n",
    "                \"What is my name?\" # Tests memory\n",
    "            ]\n",
    "            for ex in examples:\n",
    "                print(\"\\n>>\", ex)\n",
    "                try:\n",
    "                    print(agent.run(ex))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during agent execution: {e}\")\n",
    "                finally:\n",
    "                    # *** FIX: Save memory in 'finally' to ensure it saves even on error ***\n",
    "                    save_memory(memory)\n",
    "            print(\"üíæ Memory saved for examples.\")\n",
    "        elif choice == \"5\":\n",
    "            if os.path.exists(MEMORY_FILE):\n",
    "                os.remove(MEMORY_FILE)\n",
    "                print(\"üóëÔ∏è Cleared persistent memory file.\")\n",
    "            memory.clear() # Clear the in-RAM version\n",
    "            # Re-initialize empty memory\n",
    "            memory = load_memory()\n",
    "            # We must re-initialize the agent so it gets the new empty memory object\n",
    "            agent = initialize_agent(\n",
    "                tools=[calc_tool, rag_tool_wrapper],\n",
    "                llm=agent_llm,\n",
    "                agent_type=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                verbose=True,\n",
    "                max_iterations=4,\n",
    "                memory=memory,\n",
    "                agent_kwargs=agent_kwargs,\n",
    "                handle_parsing_errors=True\n",
    "            )\n",
    "            print(\"üß† In-memory history cleared and agent re-initialized.\")\n",
    "        elif choice == \"6\":\n",
    "            print(\"Goodbye.\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Try again.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
